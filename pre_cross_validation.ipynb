{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "nltk.download('stopwords') #language package for english\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga y preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete= pd.read_csv('dataset.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario de mapeo\n",
    "mapping = {'Human-Generated-Text': 0, 'AI-Generated-Text': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazar las clases en la columna 'class' con el mapeo\n",
    "\n",
    "df_complete['class'] = df_complete['class'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['len_text'] = df_complete['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample, _ = train_test_split(df_complete, train_size=50000, stratify=df_complete['class'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Cargamos las stopwords en inglés\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función para limpiar y procesar el texto\n",
    "def limpiar_texto(text):\n",
    "    # Verificar si el texto no es nulo o vacío\n",
    "    if isinstance(text, str):\n",
    "        # Eliminar saltos de línea y múltiples espacios\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Reemplaza saltos de línea y tabs por un espacio\n",
    "        text = text.strip()  # Elimina espacios en blanco iniciales y finales\n",
    "\n",
    "        # Convertir a palabras en minúsculas y filtrar stopwords\n",
    "        words = [\n",
    "            ps.stem(word) for word in gensim.utils.simple_preprocess(text)\n",
    "            if word not in gensim.parsing.preprocessing.STOPWORDS and word not in stop_words\n",
    "        ]\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return None  # Devuelve None si el texto es inválido\n",
    "\n",
    "# Aplicar la función de limpieza al DataFrame\n",
    "df_sample['text_cleaned'] = df_sample['text'].apply(limpiar_texto)\n",
    "\n",
    "# Eliminar filas donde el texto limpio es None o vacío\n",
    "completed = df_sample[df_sample['text_cleaned'].notnull() & (df_sample['text_cleaned'] != '')]\n",
    "\n",
    "# Reiniciar el índice del DataFrame después de eliminar las filas\n",
    "completed.reset_index(drop=True, inplace=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        king island roxystar record ab known roxi reco...\n",
       "1        kati carr british singer songwrit musician bor...\n",
       "2        peter alexand greenlaw quaif born kinn decemb ...\n",
       "3        mercuri poison type metal poison exposur mercu...\n",
       "4        parablenniu divers genu combtooth blenni atlan...\n",
       "                               ...                        \n",
       "49995    fender super reverb guitar amplifi manufactur ...\n",
       "49996    gen livia illustri plebeian famili roman repub...\n",
       "49997    fritz haber decemb januari german chemist rece...\n",
       "49998    gujarat titan franchis cricket team base ahmed...\n",
       "49999    aspen knoll estat privat commun staten island ...\n",
       "Name: text_cleaned, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = completed['text_cleaned']\n",
    "y = completed['class']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model          Scaling  Accuracy\n",
      "0           Naive Bayes       No Scaling    0.7095\n",
      "1   Logistic Regression       No Scaling    0.8926\n",
      "2         Decision Tree       No Scaling    0.7607\n",
      "3         Random Forest       No Scaling    0.8499\n",
      "4                   SVM       No Scaling    0.9045\n",
      "5               XGBoost       No Scaling    0.8881\n",
      "6                   KNN       No Scaling    0.5177\n",
      "7   Logistic Regression  Standardization    0.7651\n",
      "8         Decision Tree  Standardization    0.7606\n",
      "9         Random Forest  Standardization    0.8496\n",
      "10                  SVM  Standardization    0.7278\n",
      "11              XGBoost  Standardization    0.8881\n",
      "12                  KNN  Standardization    0.5074\n",
      "13          Naive Bayes    Normalization    0.6640\n",
      "14  Logistic Regression    Normalization    0.8979\n",
      "15        Decision Tree    Normalization    0.7604\n",
      "16        Random Forest    Normalization    0.8495\n",
      "17                  SVM    Normalization    0.7476\n",
      "18              XGBoost    Normalization    0.8881\n",
      "19                  KNN    Normalization    0.5102\n",
      "\n",
      "Best performing model:\n",
      "Model              SVM\n",
      "Scaling     No Scaling\n",
      "Accuracy        0.9045\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Prepare results DataFrame\n",
    "results = []\n",
    "\n",
    "# Vectorize the text data\n",
    "X_text_vectorized = vect.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Iterate through different scaling scenarios\n",
    "for scaling in ['No Scaling', 'Standardization', 'Normalization']:\n",
    "\n",
    "    # Scaling\n",
    "    if scaling == 'Standardization':\n",
    "        scaler = StandardScaler(with_mean=False)  # StandardScaler works with sparse input but set with_mean=False\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    elif scaling == 'Normalization':\n",
    "        scaler = MaxAbsScaler()  # MaxAbsScaler is compatible with sparse data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    # Train and evaluate models\n",
    "    for model_name, model in models.items():\n",
    "        # Skip Naive Bayes if Standardization is applied\n",
    "        if model_name == 'Naive Bayes' and scaling == 'Standardization':\n",
    "            continue\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Scaling': scaling,\n",
    "            'Accuracy': accuracy\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print summary of results\n",
    "print(results_df)\n",
    "\n",
    "# Find best performing model\n",
    "best_model = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "print(\"\\nBest performing model:\")\n",
    "print(best_model)\n",
    "\n",
    "# Optional: Save results to CSV\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente me quedo con el modelo X-Gboost, dado que los resutlados son bastantes similares a SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
