{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete= pd.read_csv('dataset.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario de mapeo\n",
    "mapping = {'Human-Generated-Text': 0, 'AI-Generated-Text': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazar las clases en la columna 'class' con el mapeo\n",
    "\n",
    "df_complete['class'] = df_complete['class'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sekhukhune I (Matsebe; circa 1814 – 13 Septemb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mount Washington is a  peak in the White Mount...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acer hillsi is an extinct maple species that w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Derrick George Sherwin (16 April 1936 – 17 Oct...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Windows shell is the graphical user interf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>OutServe Magazine was a bi-monthly digital and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>Eastern Armenia ( Arevelyan Hayastan) is the e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>Infinity Group is a private equity fund manage...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>Kattinakere (ಕಟ್ಟಿನಕೆರೆ) also called B Sagadde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>Wei Yan  (died 234), courtesy name Jiaji, was ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  class\n",
       "0       Sekhukhune I (Matsebe; circa 1814 – 13 Septemb...      1\n",
       "1       Mount Washington is a  peak in the White Mount...      1\n",
       "2       Acer hillsi is an extinct maple species that w...      1\n",
       "3       Derrick George Sherwin (16 April 1936 – 17 Oct...      0\n",
       "4       The Windows shell is the graphical user interf...      0\n",
       "...                                                   ...    ...\n",
       "299995  OutServe Magazine was a bi-monthly digital and...      1\n",
       "299996  Eastern Armenia ( Arevelyan Hayastan) is the e...      0\n",
       "299997  Infinity Group is a private equity fund manage...      1\n",
       "299998  Kattinakere (ಕಟ್ಟಿನಕೆರೆ) also called B Sagadde...      1\n",
       "299999  Wei Yan  (died 234), courtesy name Jiaji, was ...      1\n",
       "\n",
       "[300000 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que tu dataframe se llama df\n",
    "# 1. Filtrar por cada clase\n",
    "ai_data = df_complete[df_complete['class'] == 1]\n",
    "human_data = df_complete[df_complete['class'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sekhukhune I (Matsebe; circa 1814 – 13 Septemb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mount Washington is a  peak in the White Mount...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acer hillsi is an extinct maple species that w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Hetoimasia, Etimasia (Greek ἑτοιμασία, \"pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sister Elizabeth Kenny (20 September 1880 – 31...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299994</th>\n",
       "      <td>The Sentinel-class cutter, also known as Fast ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>OutServe Magazine was a bi-monthly digital and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>Infinity Group is a private equity fund manage...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>Kattinakere (ಕಟ್ಟಿನಕೆರೆ) also called B Sagadde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>Wei Yan  (died 234), courtesy name Jiaji, was ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  class\n",
       "0       Sekhukhune I (Matsebe; circa 1814 – 13 Septemb...      1\n",
       "1       Mount Washington is a  peak in the White Mount...      1\n",
       "2       Acer hillsi is an extinct maple species that w...      1\n",
       "5       The Hetoimasia, Etimasia (Greek ἑτοιμασία, \"pr...      1\n",
       "7       Sister Elizabeth Kenny (20 September 1880 – 31...      1\n",
       "...                                                   ...    ...\n",
       "299994  The Sentinel-class cutter, also known as Fast ...      1\n",
       "299995  OutServe Magazine was a bi-monthly digital and...      1\n",
       "299997  Infinity Group is a private equity fund manage...      1\n",
       "299998  Kattinakere (ಕಟ್ಟಿನಕೆರೆ) also called B Sagadde...      1\n",
       "299999  Wei Yan  (died 234), courtesy name Jiaji, was ...      1\n",
       "\n",
       "[150000 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Seleccionar 500 filas aleatorias de cada clase\n",
    "ai_sample = ai_data.sample(n=500, random_state=42)  # 500 filas de 'ai'\n",
    "human_sample = human_data.sample(n=500, random_state=42)  # 500 filas de 'human'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Concatenar las dos muestras para obtener un total de 1000 filas\n",
    "balanced_sample = pd.concat([ai_sample, human_sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. (Opcional) Barajar las filas para mezclar las clases\n",
    "balanced_sample = balanced_sample.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  class\n",
      "0    Duck Creek Aqueduct, also known as the Metamor...      0\n",
      "1    Jonathan Shapiro is a writer, producer, attorn...      0\n",
      "2    The Made V.I.P Tour is the first Chinese fan m...      0\n",
      "3    Johann Geusendam (24 February 1886 – 6 April 1...      0\n",
      "4    The discography of American ambient musician B...      1\n",
      "..                                                 ...    ...\n",
      "995  Mitică  is a fictional character who appears i...      1\n",
      "996  The 2019 MSBL season was the 31st season of th...      1\n",
      "997  Viral synapse (or virological synapse) is a  m...      0\n",
      "998  Werk-en-rust, also Werken-Rust, (Work and Rest...      1\n",
      "999  David Edward Gatten (Born February 11, 1971) i...      1\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ver el dataframe resultante\n",
    "print(balanced_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el tokenizer y el modelo\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Función para obtener embeddings\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "# Aplicar la función a la columna de texto\n",
    "df_complete['Embeddings'] = df_complete['text'].apply(get_embeddings)\n",
    "\n",
    "# Ver el DataFrame resultante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column embedding into columns with names emb_1, emb_2, ..., emb_768\n",
    "df = pd.concat([df_complete.drop(['Embeddings'], axis=1), df_complete['Embeddings'].apply(lambda x: pd.Series(x.flatten()))], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from transformers import BartTokenizer, AlbertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n",
      "The class this function is called from is 'BartTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBartTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtextattack/albert-base-v2-imdb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m AlbertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextattack/albert-base-v2-imdb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anano\\anaconda3\\envs\\final_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2208\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2206\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2219\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anano\\anaconda3\\envs\\final_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2442\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2440\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2441\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2442\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[0;32m   2444\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2447\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\anano\\anaconda3\\envs\\final_project\\Lib\\site-packages\\transformers\\models\\bart\\tokenization_bart.py:180\u001b[0m, in \u001b[0;36mBartTokenizer.__init__\u001b[1;34m(self, vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[0;32m    178\u001b[0m mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(vocab_handle)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"textattack/albert-base-v2-imdb\")\n",
    "model = AlbertForSequenceClassification.from_pretrained(\"textattack/albert-base-v2-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]\n",
    "\n",
    "# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "num_labels = len(model.config.id2label)\n",
    "model = AlbertForSequenceClassification.from_pretrained(\"textattack/albert-base-v2-imdb\", num_labels=num_labels)\n",
    "\n",
    "labels = torch.tensor([1])\n",
    "loss = model(**inputs, labels=labels).loss\n",
    "round(loss.item(), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
