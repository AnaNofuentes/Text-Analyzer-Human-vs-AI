{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import gensim # take text for clean and tokenize list of words\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "nltk.download('stopwords') #language package for english\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete= pd.read_csv('dataset.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario de mapeo\n",
    "mapping = {'Human-Generated-Text': 0, 'AI-Generated-Text': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazar las clases en la columna 'class' con el mapeo\n",
    "\n",
    "df_complete['class'] = df_complete['class'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['len_text'] = df_complete['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Inicializamos PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Cargamos las stopwords en inglés\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función para limpiar y procesar el texto\n",
    "def limpiar_texto(text):\n",
    "    # Verificar si el texto no es nulo o vacío\n",
    "    if isinstance(text, str):\n",
    "        # Eliminar saltos de línea y múltiples espacios\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Reemplaza saltos de línea y tabs por un espacio\n",
    "        text = text.strip()  # Elimina espacios en blanco iniciales y finales\n",
    "\n",
    "        # Convertir a palabras en minúsculas y filtrar stopwords\n",
    "        words = [\n",
    "            ps.stem(word) for word in gensim.utils.simple_preprocess(text)\n",
    "            if word not in gensim.parsing.preprocessing.STOPWORDS and word not in stop_words\n",
    "        ]\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return None  # Devuelve None si el texto es inválido\n",
    "\n",
    "# Aplicar la función de limpieza al DataFrame\n",
    "df_complete['text_cleaned'] = df_complete['text'].apply(limpiar_texto)\n",
    "\n",
    "# Eliminar filas donde el texto limpio es None o vacío\n",
    "completed = df_complete[df_complete['text_cleaned'].notnull() & (df_complete['text_cleaned'] != '')]\n",
    "\n",
    "# Reiniciar el índice del DataFrame después de eliminar las filas\n",
    "completed.reset_index(drop=True, inplace=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         sekhukhun matseb circa septemb known sekhukhun...\n",
       "1         mount washington peak white mountain new hamps...\n",
       "2         acer hillsi extinct mapl speci endem central a...\n",
       "3         derrick georg sherwin april octob english tele...\n",
       "4         window shell graphic user interfac microsoft w...\n",
       "                                ...                        \n",
       "299995    outserv magazin bi monthli digit print magazin...\n",
       "299996    eastern armenia arevelyan hayastan eastern par...\n",
       "299997    infin group privat equiti fund manag compani c...\n",
       "299998    kattinaker ಕಟ ನಕ call sagadd villag belgaum di...\n",
       "299999    wei yan die courtesi jiaji imperi offici serv ...\n",
       "Name: text_cleaned, Length: 300000, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = completed['text_cleaned']\n",
    "y = completed['class']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         sekhukhun matseb circa septemb known sekhukhun...\n",
       "1         mount washington peak white mountain new hamps...\n",
       "2         acer hillsi extinct mapl speci endem central a...\n",
       "3         derrick georg sherwin april octob english tele...\n",
       "4         window shell graphic user interfac microsoft w...\n",
       "                                ...                        \n",
       "299995    outserv magazin bi monthli digit print magazin...\n",
       "299996    eastern armenia arevelyan hayastan eastern par...\n",
       "299997    infin group privat equiti fund manag compani c...\n",
       "299998    kattinaker ಕಟ ನಕ call sagadd villag belgaum di...\n",
       "299999    wei yan die courtesi jiaji imperi offici serv ...\n",
       "Name: text_cleaned, Length: 300000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Prepare results DataFrame\n",
    "results = []\n",
    "\n",
    "# Vectorize the text data\n",
    "X_text_vectorized = vect.fit_transform(X)\n",
    "# Luego, usa directamente la matriz dispersa en train_test_split y en los modelos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Iterate through different scenarios\n",
    "for scaling in ['No Scaling', 'Standardization', 'Normalization']:\n",
    "\n",
    "    # Scaling\n",
    "    if scaling == 'Standardization':\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    elif scaling == 'Normalization':\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    # Train and evaluate models\n",
    "    for model_name, model in models.items():\n",
    "        # Skip Naive Bayes if standardization is applied\n",
    "        if model_name == 'Naive Bayes' and scaling == 'Standardization':\n",
    "            continue\n",
    "\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Scaling': scaling,\n",
    "            'Accuracy': accuracy\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print summary\n",
    "print(results_df)\n",
    "\n",
    "# Find best performing model\n",
    "best_model = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "print(\"\\nBest performing model:\")\n",
    "print(best_model)\n",
    "\n",
    "# Optional: Save results to CSV\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
