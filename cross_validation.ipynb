{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import gensim # take text for clean and tokenize list of words\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "nltk.download('stopwords') #language package for english\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete= pd.read_csv('dataset.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario de mapeo\n",
    "mapping = {'Human-Generated-Text': 0, 'AI-Generated-Text': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazar las clases en la columna 'class' con el mapeo\n",
    "\n",
    "df_complete['class'] = df_complete['class'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['len_text'] = df_complete['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample, _ = train_test_split(df_complete, train_size=5000, stratify=df_complete['class'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Inicializamos PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Cargamos las stopwords en inglés\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función para limpiar y procesar el texto\n",
    "def limpiar_texto(text):\n",
    "    # Verificar si el texto no es nulo o vacío\n",
    "    if isinstance(text, str):\n",
    "        # Eliminar saltos de línea y múltiples espacios\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Reemplaza saltos de línea y tabs por un espacio\n",
    "        text = text.strip()  # Elimina espacios en blanco iniciales y finales\n",
    "\n",
    "        # Convertir a palabras en minúsculas y filtrar stopwords\n",
    "        words = [\n",
    "            ps.stem(word) for word in gensim.utils.simple_preprocess(text)\n",
    "            if word not in gensim.parsing.preprocessing.STOPWORDS and word not in stop_words\n",
    "        ]\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return None  # Devuelve None si el texto es inválido\n",
    "\n",
    "# Aplicar la función de limpieza al DataFrame\n",
    "df_sample['text_cleaned'] = df_sample['text'].apply(limpiar_texto)\n",
    "\n",
    "# Eliminar filas donde el texto limpio es None o vacío\n",
    "completed = df_sample[df_sample['text_cleaned'].notnull() & (df_sample['text_cleaned'] != '')]\n",
    "\n",
    "# Reiniciar el índice del DataFrame después de eliminar las filas\n",
    "completed.reset_index(drop=True, inplace=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       bank fish fish place bank river bed steepli in...\n",
       "1       simion mehedinți octob decemb romanian symboli...\n",
       "2       palmer model carbin singl shot bolt action rif...\n",
       "3       coronari catheter minim invas procedur access ...\n",
       "4       david hearn februari march lawyer polit figur ...\n",
       "                              ...                        \n",
       "4995    lochmaea suturali commonli refer sutur snail s...\n",
       "4996    stearn wharf pier southern end embarcadero san...\n",
       "4997    kingskerswel king carswel king kerswel villag ...\n",
       "4998    deck arch bridg southern hakkōda mountain citi...\n",
       "4999    joseph jo montferrand born joseph favr octob o...\n",
       "Name: text_cleaned, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = completed['text_cleaned']\n",
    "y = completed['class']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       japanes navi offic highest rank rear admir hor...\n",
       "1       jesu bless christ bless manado malay kristu se...\n",
       "2       wilfr turner octob januari english novelist cr...\n",
       "3       budweis american style pale lager produc anheu...\n",
       "4       jona salk hall univers pittsburgh pennsylvania...\n",
       "                              ...                        \n",
       "1995    theta boöti latin boöti star northern constel ...\n",
       "1996    christian africa arriv egypt middl st centuri ...\n",
       "1997    discographi american electronica project owl c...\n",
       "1998    richard elmer rick bartow decemb april nativ a...\n",
       "1999    adam fairclough born british historian author ...\n",
       "Name: text_cleaned, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Prepare results DataFrame\n",
    "results = []\n",
    "\n",
    "# Vectorize the text data\n",
    "X_text_vectorized = vect.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Iterate through different scaling scenarios\n",
    "for scaling in ['No Scaling', 'Standardization', 'Normalization']:\n",
    "\n",
    "    # Scaling\n",
    "    if scaling == 'Standardization':\n",
    "        scaler = StandardScaler(with_mean=False)  # StandardScaler works with sparse input but set with_mean=False\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    elif scaling == 'Normalization':\n",
    "        scaler = MaxAbsScaler()  # MaxAbsScaler is compatible with sparse data\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    # Train and evaluate models\n",
    "    for model_name, model in models.items():\n",
    "        # Skip Naive Bayes if Standardization is applied\n",
    "        if model_name == 'Naive Bayes' and scaling == 'Standardization':\n",
    "            continue\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Scaling': scaling,\n",
    "            'Accuracy': accuracy\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print summary of results\n",
    "print(results_df)\n",
    "\n",
    "# Find best performing model\n",
    "best_model = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "print(\"\\nBest performing model:\")\n",
    "print(best_model)\n",
    "\n",
    "# Optional: Save results to CSV\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
