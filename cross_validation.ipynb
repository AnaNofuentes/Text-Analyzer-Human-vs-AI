{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import gensim # take text for clean and tokenize list of words\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "nltk.download('stopwords') #language package for english\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete= pd.read_csv('dataset.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario de mapeo\n",
    "mapping = {'Human-Generated-Text': 0, 'AI-Generated-Text': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazar las clases en la columna 'class' con el mapeo\n",
    "\n",
    "df_complete['class'] = df_complete['class'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['len_text'] = df_complete['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Inicializamos PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Cargamos las stopwords en inglés\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función para limpiar y procesar el texto\n",
    "def limpiar_texto(text):\n",
    "    # Verificar si el texto no es nulo o vacío\n",
    "    if isinstance(text, str):\n",
    "        # Eliminar saltos de línea y múltiples espacios\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Reemplaza saltos de línea y tabs por un espacio\n",
    "        text = text.strip()  # Elimina espacios en blanco iniciales y finales\n",
    "\n",
    "        # Convertir a palabras en minúsculas y filtrar stopwords\n",
    "        words = [\n",
    "            ps.stem(word) for word in gensim.utils.simple_preprocess(text)\n",
    "            if word not in gensim.parsing.preprocessing.STOPWORDS and word not in stop_words\n",
    "        ]\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return None  # Devuelve None si el texto es inválido\n",
    "\n",
    "# Aplicar la función de limpieza al DataFrame\n",
    "df_complete['text_cleaned'] = df_complete['text'].apply(limpiar_texto)\n",
    "\n",
    "# Eliminar filas donde el texto limpio es None o vacío\n",
    "completed = df_complete[df_complete['text_cleaned'].notnull() & (df_complete['text_cleaned'] != '')]\n",
    "\n",
    "# Reiniciar el índice del DataFrame después de eliminar las filas\n",
    "completed.reset_index(drop=True, inplace=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len_text</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sekhukhune I (Matsebe; circa 1814 – 13 Septemb...</td>\n",
       "      <td>1256</td>\n",
       "      <td>sekhukhun matseb circa septemb known sekhukhun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mount Washington is a  peak in the White Mount...</td>\n",
       "      <td>628</td>\n",
       "      <td>mount washington peak white mountain new hamps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acer hillsi is an extinct maple species that w...</td>\n",
       "      <td>694</td>\n",
       "      <td>acer hillsi extinct mapl speci endem central a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Derrick George Sherwin (16 April 1936 – 17 Oct...</td>\n",
       "      <td>945</td>\n",
       "      <td>derrick georg sherwin april octob english tele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Windows shell is the graphical user interf...</td>\n",
       "      <td>1143</td>\n",
       "      <td>window shell graphic user interfac microsoft w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>OutServe Magazine was a bi-monthly digital and...</td>\n",
       "      <td>734</td>\n",
       "      <td>outserv magazin bi monthli digit print magazin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>Eastern Armenia ( Arevelyan Hayastan) is the e...</td>\n",
       "      <td>1143</td>\n",
       "      <td>eastern armenia arevelyan hayastan eastern par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>Infinity Group is a private equity fund manage...</td>\n",
       "      <td>673</td>\n",
       "      <td>infin group privat equiti fund manag compani c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>Kattinakere (ಕಟ್ಟಿನಕೆರೆ) also called B Sagadde...</td>\n",
       "      <td>648</td>\n",
       "      <td>kattinaker ಕಟ ನಕ call sagadd villag belgaum di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>Wei Yan  (died 234), courtesy name Jiaji, was ...</td>\n",
       "      <td>742</td>\n",
       "      <td>wei yan die courtesi jiaji imperi offici serv ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  len_text  \\\n",
       "0       Sekhukhune I (Matsebe; circa 1814 – 13 Septemb...      1256   \n",
       "1       Mount Washington is a  peak in the White Mount...       628   \n",
       "2       Acer hillsi is an extinct maple species that w...       694   \n",
       "3       Derrick George Sherwin (16 April 1936 – 17 Oct...       945   \n",
       "4       The Windows shell is the graphical user interf...      1143   \n",
       "...                                                   ...       ...   \n",
       "299995  OutServe Magazine was a bi-monthly digital and...       734   \n",
       "299996  Eastern Armenia ( Arevelyan Hayastan) is the e...      1143   \n",
       "299997  Infinity Group is a private equity fund manage...       673   \n",
       "299998  Kattinakere (ಕಟ್ಟಿನಕೆರೆ) also called B Sagadde...       648   \n",
       "299999  Wei Yan  (died 234), courtesy name Jiaji, was ...       742   \n",
       "\n",
       "                                             text_cleaned  \n",
       "0       sekhukhun matseb circa septemb known sekhukhun...  \n",
       "1       mount washington peak white mountain new hamps...  \n",
       "2       acer hillsi extinct mapl speci endem central a...  \n",
       "3       derrick georg sherwin april octob english tele...  \n",
       "4       window shell graphic user interfac microsoft w...  \n",
       "...                                                   ...  \n",
       "299995  outserv magazin bi monthli digit print magazin...  \n",
       "299996  eastern armenia arevelyan hayastan eastern par...  \n",
       "299997  infin group privat equiti fund manag compani c...  \n",
       "299998  kattinaker ಕಟ ನಕ call sagadd villag belgaum di...  \n",
       "299999  wei yan die courtesi jiaji imperi offici serv ...  \n",
       "\n",
       "[300000 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = completed.drop('class', axis=1)\n",
    "y = completed['class']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         sekhukhun matseb circa septemb known sekhukhun...\n",
       "1         mount washington peak white mountain new hamps...\n",
       "2         acer hillsi extinct mapl speci endem central a...\n",
       "3         derrick georg sherwin april octob english tele...\n",
       "4         window shell graphic user interfac microsoft w...\n",
       "                                ...                        \n",
       "299995    outserv magazin bi monthli digit print magazin...\n",
       "299996    eastern armenia arevelyan hayastan eastern par...\n",
       "299997    infin group privat equiti fund manag compani c...\n",
       "299998    kattinaker ಕಟ ನಕ call sagadd villag belgaum di...\n",
       "299999    wei yan die courtesi jiaji imperi offici serv ...\n",
       "Name: text_cleaned, Length: 300000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Function to remove outliers\n",
    "def remove_outliers(df, columns, z_threshold=3):\n",
    "    columns = [col for col in columns if col in df.columns]\n",
    "    return df[(np.abs(stats.zscore(df[columns])) < z_threshold).all(axis=1)]\n",
    "\n",
    "# Define different feature sets\n",
    "numeric_features = ['len_text']\n",
    "text_features = ['text_cleaned']\n",
    "\n",
    "feature_sets = {\n",
    "    'All Features': numeric_features + text_features,\n",
    "    'Numeric Only': numeric_features,\n",
    "    'Text Only': text_features\n",
    "}\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Prepare results DataFrame\n",
    "results = []\n",
    "\n",
    "# Iterate through different scenarios\n",
    "for outliers in ['With Outliers', 'Without Outliers']:\n",
    "    for scaling in ['No Scaling', 'Standardization', 'Normalization']:\n",
    "        for feature_set_name, features in feature_sets.items():\n",
    "            # Prepare data\n",
    "            X_subset = X[features]\n",
    "\n",
    "            if 'text_cleaned' in features:\n",
    "                # Vectorize the text data\n",
    "                X_text_vectorized = vect.fit_transform(X['text_cleaned']).toarray()\n",
    "                X_subset = pd.DataFrame(X_text_vectorized, index=X.index)\n",
    "            \n",
    "            # If using numeric data, include the numeric features\n",
    "            if 'len_text' in features and 'text_cleaned' in features:\n",
    "                X_subset = pd.concat([X['len_text'].reset_index(drop=True), pd.DataFrame(X_text_vectorized)], axis=1)\n",
    "\n",
    "            # Remove outliers if applicable\n",
    "            if outliers == 'Without Outliers' and 'len_text' in features:\n",
    "                X_subset = remove_outliers(X_subset, numeric_features)\n",
    "                y_subset = y[X_subset.index]\n",
    "            else:\n",
    "                y_subset = y\n",
    "\n",
    "            # Split the data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Scaling\n",
    "            if scaling == 'Standardization':\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "            elif scaling == 'Normalization':\n",
    "                scaler = MinMaxScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "            else:\n",
    "                X_train_scaled = X_train\n",
    "                X_test_scaled = X_test\n",
    "\n",
    "            # Train and evaluate models\n",
    "            for model_name, model in models.items():\n",
    "                # Skip Naive Bayes if standardization is applied\n",
    "                if model_name == 'Naive Bayes' and scaling == 'Standardization':\n",
    "                    continue\n",
    "\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Outliers': outliers,\n",
    "                    'Scaling': scaling,\n",
    "                    'Feature Set': feature_set_name,\n",
    "                    'Accuracy': accuracy\n",
    "                })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print summary\n",
    "print(results_df)\n",
    "\n",
    "# Find best performing model\n",
    "best_model = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "print(\"\\nBest performing model:\")\n",
    "print(best_model)\n",
    "\n",
    "# Optional: Save results to CSV\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
